# Multi-Language Translation Leaderboard API

A research platform for evaluating and ranking AI translation models across 5 languages using both BLEU and BERTScore metrics. Features real-time submissions, dynamic leaderboards, and comprehensive multi-language evaluation.

> **📍 Documentation Location**: This documentation is located in `docs/leaderboard/` to keep it organized separately from the main repository's other functionalities.

## ✅ **CURRENT STATUS: WORKING END-TO-END**

### 🎯 **Working URLs:**
- **Frontend**: http://localhost:3002
- **Evaluations Page**: http://localhost:3002/evaluations ✅ **LIVE**
- **Backend API**: http://localhost:5001
- **Health Check**: http://localhost:5001/health

### 🔍 **Available Leaderboards:**
Based on live data from the API:
- **Spanish**: BLEU + BERTScore ✅
- **Arabic**: BLEU + BERTScore ✅  
- **Chinese**: BLEU + BERTScore ✅
- **Japanese**: BLEU + BERTScore ✅
- **Korean**: BLEU ✅

**Total**: 10 active leaderboards with real submission data

## Key Features

- **5 Languages**: Spanish, Japanese, Arabic, Chinese, Korean
- **2 Evaluation Metrics**: BLEU (exact matching) + BERTScore (semantic similarity)  
- **Real-time Leaderboards**: Dynamic rankings with live updates
- **Model Submission**: Easy CSV upload interface for translation results
- **FLORES+ Integration**: Industry-standard benchmark datasets
- **Research-Grade Evaluation**: Comprehensive metric comparison

## Quick Start

### Prerequisites
- Docker and Docker Compose
- Node.js and npm
- Git

### Setup and Run

1. **Clone and Start Backend**
   ```bash
   git clone <repository-url>
   cd Autonomous-Intelligence-chatbots-leaderboard-api
   cd backend
   docker-compose up --build -d
   ```

2. **Start Frontend**
   ```bash
   cd ../frontend
   npm install
   npm start  # Will auto-select available port (3002)
   ```

3. **Access Applications**
   - **Evaluations**: http://localhost:3002/evaluations
   - **Backend API**: http://localhost:5001
   - **Health Check**: http://localhost:5001/health

## Evaluation Metrics Comparison

| Metric | Spanish | Arabic | Korean | Japanese | Chinese | Advantage |
|--------|---------|--------|--------|----------|---------|-----------|
| **BLEU** | ~0.40 | ~0.10 | ~0.13 | ~0.00 | ~0.00 | Fast, established standard |
| **BERTScore** | ~0.65 | ~0.65 | ~0.60 | ~0.62 | ~0.62 | Semantic similarity, works for all languages |

**Key Insight**: BERTScore provides meaningful evaluation for all languages, while BLEU struggles with Asian languages due to structural differences.

## How It Works

1. **Browse Leaderboards**: View current model rankings by language and metric at `/evaluations`
2. **Submit Your Model**: Upload CSV with translation results via "Submit Model to Leaderboard" button
3. **Get Evaluated**: System calculates BLEU/BERTScore against FLORES+ references  
4. **See Results**: Your model appears in real-time leaderboard rankings

## Testing & Demo

**Quick Demo**: See `TESTING_GUIDE.md` for complete instructions

### 📁 Test Files Available (Complete Coverage)

**BLEU Test Files** (`test_files/`):
- `demo_test_spanish.csv` - Spanish BLEU evaluation
- `demo_test_japanese.csv` - Japanese BLEU evaluation  
- `demo_test_korean.csv` - Korean BLEU evaluation
- `demo_test_arabic.csv` - Arabic BLEU evaluation
- `demo_test_chinese.csv` - Chinese BLEU evaluation

**BERTScore Test Files** (`test_files/`):
- `demo_test_spanish_bertscore.csv` - Spanish BERTScore evaluation
- `demo_test_japanese_bertscore.csv` - Japanese BERTScore evaluation
- `demo_test_korean_bertscore.csv` - Korean BERTScore evaluation  
- `demo_test_arabic_bertscore.csv` - Arabic BERTScore evaluation
- `demo_test_chinese_bertscore.csv` - Chinese BERTScore evaluation

**Score Range Examples**:
- `test_high_score.csv` - High-quality translation examples
- `test_medium_score.csv` - Medium-quality translation examples  
- `test_low_score.csv` - Low-quality translation examples

**Total**: 13 test files covering all 10 leaderboard combinations + quality examples

## API Endpoints

### Submit Model
```bash
POST http://localhost:5000/public/submit_model
{
  "benchmarkDatasetName": "flores_spanish_translation",
  "modelName": "my-model-v1", 
  "modelResults": ["Translation 1", "Translation 2"],
  "sentence_ids": [0, 1]
}
```

### Get Source Sentences
```bash
GET http://localhost:5000/public/get_source_sentences?dataset_name=flores_spanish_translation&count=5
```

### Get Leaderboard (Live Data)
```bash
GET http://localhost:5000/public/get_leaderboard
```

**Response includes 9 active leaderboards:**
- `flores_spanish_translation` (BLEU)
- `flores_spanish_translation_bertscore` (BERTScore)
- `flores_arabic_translation` (BLEU)
- `flores_arabic_translation_bertscore` (BERTScore)
- `flores_chinese_translation` (BLEU)
- `flores_chinese_translation_bertscore` (BERTScore)
- `flores_japanese_translation` (BLEU)
- `flores_japanese_translation_bertscore` (BERTScore)
- `flores_korean_translation` (BLEU)

## Development

### Workflow
1. **Make Changes**: Edit code in your preferred IDE
2. **Test Locally**: Use endpoints above to verify functionality  
3. **Commit & Push**: 
   ```bash
   git add .
   git commit -m "Your change description"
   git push origin chatbots-leaderboard-api
   ```

### Architecture
- **Backend**: Flask API with MySQL database (runs on port 8000)
- **Frontend**: React with dynamic leaderboard components (runs on port 3002)
- **Evaluation**: BLEU (nltk) + BERTScore (bert-base-multilingual-cased)
- **Data**: FLORES+ benchmark datasets via Hugging Face

## Configuration

### Port Configuration
- **Backend**: Accessible at http://localhost:8000 (Docker maps container port 5000 to host port 8000)
- **Frontend**: Accessible at http://localhost:3002 (auto-selected to avoid conflicts)
- **Database**: MySQL on port 3307 (mapped from container port 3306)

### Environment Variables
Ensure your backend `.env` file includes:
```bash
# Database configuration
DB_NAME=agents
DB_HOST=db
DB_USER=root
DB_PASSWORD=

# API Keys
OPENAI_API_KEY=your_openai_key_here
HF_TOKEN=your_huggingface_token_here
```

## 🐛 **Known Issue: Missing Leaderboards**
Currently only Spanish-BLEU leaderboard is displaying on the frontend, despite having data for all 9 leaderboards. This is a frontend display issue being investigated.

## Documentation

- **`TESTING_GUIDE.md`**: Comprehensive testing instructions + demo guide
- **`CODEBASE_SETUP.md`**: Development environment setup

---

# Original Implementation Documentation

## Overview
Fully functional leaderboard API for automated model submissions with real BLEU evaluation. 

## 🎯 API Specification Compliance

**Original Spec:**
- Input: `benchmark dataset name, model_name, model_results`
- Output: `Boolean (success or failure), Score`

**Implementation:** ✅ **FULLY COMPLIANT** (+ enhanced for proper evaluation)

## 🔄 Complete Workflow

### 1. Get Source Sentences
```bash
GET /public/get_source_sentences?count=3&start_idx=0
```
**Response:**
```json
{
  "success": true,
  "sentence_ids": [0, 1, 2],
  "source_sentences": ["English sentence 1", "English sentence 2", "English sentence 3"],
  "dataset": "flores_spanish_translation",
  "total_available": 1012
}
```

### 2. Submit Model Results (Spec-Compliant)
```bash
POST /public/submit_model
```
**Request:**
```json
{
  "benchmarkDatasetName": "flores_spanish_translation",
  "modelName": "my-model-v1",
  "modelResults": ["Spanish translation 1", "Spanish translation 2", "Spanish translation 3"],
  "sentence_ids": [0, 1, 2]
}
```

**Response (Exact Spec Match):**
```json
{
  "success": true,
  "score": 0.7050
}
```

## 🧪 Verification Results

| Translation Quality | BLEU Score | Status |
|-------------------|------------|---------|
| High-quality (near-perfect) | **0.7050** | ✅ Excellent |
| Medium-quality (good) | **0.1154** | ✅ Fair |
| Low-quality (wrong) | **0.0002** | ✅ Appropriately low |

**✅ Scores follow expected pattern: high > medium > low**

## 🚀 Key Features

- ✅ **No API keys required** (public access)
- ✅ **Real BLEU evaluation** using Angela's `get_bleu()` function
- ✅ **Option C flow** for meaningful, comparable scores
- ✅ **Database persistence** for leaderboard functionality
- ✅ **Proper validation** and error handling
- ✅ **Spec-compliant output** format

## 🛠 Database Schema

Three tables automatically created via migration:
- `benchmark_datasets` - Available benchmarks
- `model_submissions` - Track submissions  
- `evaluation_results` - Store scores

## 📋 Angela's Endpoint Status

**`/spanish-gpt-evaluation`** - ✅ **FIXED & WORKING**
- Updated OpenAI API compatibility
- Real-time GPT translation + BLEU evaluation
- Produces ~0.4 BLEU scores as expected

## 🧪 Testing

```bash
# Comprehensive verification (recommended)
python backend/tests/test_submit_model_verification.py

# Both endpoints test
python backend/tests/test_both_endpoints.py
```

## 💡 Flow Update

**Problem:** Random translations → meaningless 0.0 BLEU scores  
**Solution:** Users translate specific benchmark sentences → comparable, meaningful scores

**Flow ensures:**
- Fair comparison (same source content)
- Meaningful BLEU scores
- Leaderboard-ready results

## 🔧 Environment Setup

Required in `.env`:
```bash
# Hugging Face token for FLORES+ dataset access
HF_TOKEN=hf_your_token_here

# OpenAI for Angela's endpoint  
OPENAI_API_KEY=sk-your-key-here
```

## 🚀 Quick Start

```bash
# 1. Start containers
cd backend && docker-compose up -d

# 2. Run migration (if needed)
docker exec -it anote-backend python database/run_migration.py

# 3. Test the API
python backend/tests/test_submit_model_verification.py
```

## ✅ Production Status

- ✅ Real BLEU evaluation integrated 
- ✅ Spec-compliant API  
- ✅ Database persistence
- ✅ Comprehensive test coverage
- ✅ Angela's endpoint fixed
- ✅ Documentation complete
- ✅ Ready for frontend integration

## 🎯 Results Summary

**High-quality translation example:**
- Source: *"We now have 4-month-old mice that are non-diabetic that used to be diabetic," he added.*
- Reference: *«Actualmente, tenemos ratones de cuatro meses de edad que antes solían ser diabéticos y que ya no lo son», agregó.*
- User translation: *«Actualmente, tenemos ratones de cuatro meses de edad que antes solían ser diabéticos y que ya no lo son», agregó.*
- **BLEU Score: 0.7050** ✅

The API successfully distinguishes between high-quality and low-quality translations, making it suitable for leaderboard ranking. 