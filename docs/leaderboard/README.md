# Multi-Language Translation Leaderboard API

A research platform for evaluating and ranking AI translation models across 5 languages using both BLEU and BERTScore metrics. Features real-time submissions, dynamic leaderboards, and comprehensive multi-language evaluation.

> **ğŸ“ Documentation Location**: This documentation is located in `docs/leaderboard/` to keep it organized separately from the main repository's other functionalities.

## âœ… **CURRENT STATUS: WORKING END-TO-END**

### ğŸ¯ **Working URLs:**
- **Frontend**: http://localhost:3002
- **Evaluations Page**: http://localhost:3002/evaluations âœ… **LIVE**
- **Backend API**: http://localhost:5001
- **Health Check**: http://localhost:5001/health

### ğŸ” **Available Leaderboards:**
Based on live data from the API:
- **Spanish**: BLEU + BERTScore âœ…
- **Arabic**: BLEU + BERTScore âœ…  
- **Chinese**: BLEU + BERTScore âœ…
- **Japanese**: BLEU + BERTScore âœ…
- **Korean**: BLEU âœ…

**Total**: 10 active leaderboards with real submission data

## Key Features

- **5 Languages**: Spanish, Japanese, Arabic, Chinese, Korean
- **2 Evaluation Metrics**: BLEU (exact matching) + BERTScore (semantic similarity)  
- **Real-time Leaderboards**: Dynamic rankings with live updates
- **Model Submission**: Easy CSV upload interface for translation results
- **FLORES+ Integration**: Industry-standard benchmark datasets
- **Research-Grade Evaluation**: Comprehensive metric comparison

## Quick Start

### Prerequisites
- Docker and Docker Compose
- Node.js and npm
- Git

### Setup and Run

1. **Clone and Start Backend**
   ```bash
   git clone <repository-url>
   cd Autonomous-Intelligence-chatbots-leaderboard-api
   cd backend
   docker-compose up --build -d
   ```

2. **Start Frontend**
   ```bash
   cd ../frontend
   npm install
   npm start  # Will auto-select available port (3002)
   ```

3. **Access Applications**
   - **Evaluations**: http://localhost:3002/evaluations
   - **Backend API**: http://localhost:5001
   - **Health Check**: http://localhost:5001/health

## Evaluation Metrics Comparison

| Metric | Spanish | Arabic | Korean | Japanese | Chinese | Advantage |
|--------|---------|--------|--------|----------|---------|-----------|
| **BLEU** | ~0.40 | ~0.10 | ~0.13 | ~0.00 | ~0.00 | Fast, established standard |
| **BERTScore** | ~0.65 | ~0.65 | ~0.60 | ~0.62 | ~0.62 | Semantic similarity, works for all languages |

**Key Insight**: BERTScore provides meaningful evaluation for all languages, while BLEU struggles with Asian languages due to structural differences.

## How It Works

1. **Browse Leaderboards**: View current model rankings by language and metric at `/evaluations`
2. **Submit Your Model**: Upload CSV with translation results via "Submit Model to Leaderboard" button
3. **Get Evaluated**: System calculates BLEU/BERTScore against FLORES+ references  
4. **See Results**: Your model appears in real-time leaderboard rankings

## Testing & Demo

**Quick Demo**: See `TESTING_GUIDE.md` for complete instructions

### ğŸ“ Test Files Available (Complete Coverage)

**BLEU Test Files** (`test_files/`):
- `demo_test_spanish.csv` - Spanish BLEU evaluation
- `demo_test_japanese.csv` - Japanese BLEU evaluation  
- `demo_test_korean.csv` - Korean BLEU evaluation
- `demo_test_arabic.csv` - Arabic BLEU evaluation
- `demo_test_chinese.csv` - Chinese BLEU evaluation

**BERTScore Test Files** (`test_files/`):
- `demo_test_spanish_bertscore.csv` - Spanish BERTScore evaluation
- `demo_test_japanese_bertscore.csv` - Japanese BERTScore evaluation
- `demo_test_korean_bertscore.csv` - Korean BERTScore evaluation  
- `demo_test_arabic_bertscore.csv` - Arabic BERTScore evaluation
- `demo_test_chinese_bertscore.csv` - Chinese BERTScore evaluation

**Score Range Examples**:
- `test_high_score.csv` - High-quality translation examples
- `test_medium_score.csv` - Medium-quality translation examples  
- `test_low_score.csv` - Low-quality translation examples

**Total**: 13 test files covering all 10 leaderboard combinations + quality examples

## API Endpoints

### Submit Model
```bash
POST http://localhost:5000/public/submit_model
{
  "benchmarkDatasetName": "flores_spanish_translation",
  "modelName": "my-model-v1", 
  "modelResults": ["Translation 1", "Translation 2"],
  "sentence_ids": [0, 1]
}
```

### Get Source Sentences
```bash
GET http://localhost:5000/public/get_source_sentences?dataset_name=flores_spanish_translation&count=5
```

### Get Leaderboard (Live Data)
```bash
GET http://localhost:5000/public/get_leaderboard
```

**Response includes 9 active leaderboards:**
- `flores_spanish_translation` (BLEU)
- `flores_spanish_translation_bertscore` (BERTScore)
- `flores_arabic_translation` (BLEU)
- `flores_arabic_translation_bertscore` (BERTScore)
- `flores_chinese_translation` (BLEU)
- `flores_chinese_translation_bertscore` (BERTScore)
- `flores_japanese_translation` (BLEU)
- `flores_japanese_translation_bertscore` (BERTScore)
- `flores_korean_translation` (BLEU)

## Development

### Workflow
1. **Make Changes**: Edit code in your preferred IDE
2. **Test Locally**: Use endpoints above to verify functionality  
3. **Commit & Push**: 
   ```bash
   git add .
   git commit -m "Your change description"
   git push origin chatbots-leaderboard-api
   ```

### Architecture
- **Backend**: Flask API with MySQL database (runs on port 8000)
- **Frontend**: React with dynamic leaderboard components (runs on port 3002)
- **Evaluation**: BLEU (nltk) + BERTScore (bert-base-multilingual-cased)
- **Data**: FLORES+ benchmark datasets via Hugging Face

## Configuration

### Port Configuration
- **Backend**: Accessible at http://localhost:8000 (Docker maps container port 5000 to host port 8000)
- **Frontend**: Accessible at http://localhost:3002 (auto-selected to avoid conflicts)
- **Database**: MySQL on port 3307 (mapped from container port 3306)

### Environment Variables
Ensure your backend `.env` file includes:
```bash
# Database configuration
DB_NAME=agents
DB_HOST=db
DB_USER=root
DB_PASSWORD=

# API Keys
OPENAI_API_KEY=your_openai_key_here
HF_TOKEN=your_huggingface_token_here
```

## ğŸ› **Known Issue: Missing Leaderboards**
Currently only Spanish-BLEU leaderboard is displaying on the frontend, despite having data for all 9 leaderboards. This is a frontend display issue being investigated.

## Documentation

- **`TESTING_GUIDE.md`**: Comprehensive testing instructions + demo guide
- **`CODEBASE_SETUP.md`**: Development environment setup

---

# Original Implementation Documentation

## Overview
Fully functional leaderboard API for automated model submissions with real BLEU evaluation. 

## ğŸ¯ API Specification Compliance

**Original Spec:**
- Input: `benchmark dataset name, model_name, model_results`
- Output: `Boolean (success or failure), Score`

**Implementation:** âœ… **FULLY COMPLIANT** (+ enhanced for proper evaluation)

## ğŸ”„ Complete Workflow

### 1. Get Source Sentences
```bash
GET /public/get_source_sentences?count=3&start_idx=0
```
**Response:**
```json
{
  "success": true,
  "sentence_ids": [0, 1, 2],
  "source_sentences": ["English sentence 1", "English sentence 2", "English sentence 3"],
  "dataset": "flores_spanish_translation",
  "total_available": 1012
}
```

### 2. Submit Model Results (Spec-Compliant)
```bash
POST /public/submit_model
```
**Request:**
```json
{
  "benchmarkDatasetName": "flores_spanish_translation",
  "modelName": "my-model-v1",
  "modelResults": ["Spanish translation 1", "Spanish translation 2", "Spanish translation 3"],
  "sentence_ids": [0, 1, 2]
}
```

**Response (Exact Spec Match):**
```json
{
  "success": true,
  "score": 0.7050
}
```

## ğŸ§ª Verification Results

| Translation Quality | BLEU Score | Status |
|-------------------|------------|---------|
| High-quality (near-perfect) | **0.7050** | âœ… Excellent |
| Medium-quality (good) | **0.1154** | âœ… Fair |
| Low-quality (wrong) | **0.0002** | âœ… Appropriately low |

**âœ… Scores follow expected pattern: high > medium > low**

## ğŸš€ Key Features

- âœ… **No API keys required** (public access)
- âœ… **Real BLEU evaluation** using Angela's `get_bleu()` function
- âœ… **Option C flow** for meaningful, comparable scores
- âœ… **Database persistence** for leaderboard functionality
- âœ… **Proper validation** and error handling
- âœ… **Spec-compliant output** format

## ğŸ›  Database Schema

Three tables automatically created via migration:
- `benchmark_datasets` - Available benchmarks
- `model_submissions` - Track submissions  
- `evaluation_results` - Store scores

## ğŸ“‹ Angela's Endpoint Status

**`/spanish-gpt-evaluation`** - âœ… **FIXED & WORKING**
- Updated OpenAI API compatibility
- Real-time GPT translation + BLEU evaluation
- Produces ~0.4 BLEU scores as expected

## ğŸ§ª Testing

```bash
# Comprehensive verification (recommended)
python backend/tests/test_submit_model_verification.py

# Both endpoints test
python backend/tests/test_both_endpoints.py
```

## ğŸ’¡ Flow Update

**Problem:** Random translations â†’ meaningless 0.0 BLEU scores  
**Solution:** Users translate specific benchmark sentences â†’ comparable, meaningful scores

**Flow ensures:**
- Fair comparison (same source content)
- Meaningful BLEU scores
- Leaderboard-ready results

## ğŸ”§ Environment Setup

Required in `.env`:
```bash
# Hugging Face token for FLORES+ dataset access
HF_TOKEN=hf_your_token_here

# OpenAI for Angela's endpoint  
OPENAI_API_KEY=sk-your-key-here
```

## ğŸš€ Quick Start

```bash
# 1. Start containers
cd backend && docker-compose up -d

# 2. Run migration (if needed)
docker exec -it anote-backend python database/run_migration.py

# 3. Test the API
python backend/tests/test_submit_model_verification.py
```

## âœ… Production Status

- âœ… Real BLEU evaluation integrated 
- âœ… Spec-compliant API  
- âœ… Database persistence
- âœ… Comprehensive test coverage
- âœ… Angela's endpoint fixed
- âœ… Documentation complete
- âœ… Ready for frontend integration

## ğŸ¯ Results Summary

**High-quality translation example:**
- Source: *"We now have 4-month-old mice that are non-diabetic that used to be diabetic," he added.*
- Reference: *Â«Actualmente, tenemos ratones de cuatro meses de edad que antes solÃ­an ser diabÃ©ticos y que ya no lo sonÂ», agregÃ³.*
- User translation: *Â«Actualmente, tenemos ratones de cuatro meses de edad que antes solÃ­an ser diabÃ©ticos y que ya no lo sonÂ», agregÃ³.*
- **BLEU Score: 0.7050** âœ…

The API successfully distinguishes between high-quality and low-quality translations, making it suitable for leaderboard ranking. 