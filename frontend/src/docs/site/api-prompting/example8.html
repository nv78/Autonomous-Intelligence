
<!DOCTYPE html>

<html class="no-js" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<link href="https://docs.Anote.ai/api-prompting/example8.html" rel="canonical"/>
<link href="example7.html" rel="prev"/>
<link href="../assets/logo.png" rel="icon"/>
<meta content="mkdocs-1.4.3, mkdocs-material-9.1.15" name="generator"/>
<title>Evaluation - Private Chatbot</title>
<link href="../assets/stylesheets/main.26e3688c.min.css" rel="stylesheet"/>
<link href="../assets/stylesheets/palette.ecc896b0.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
<link href="../assets/_mkdocstrings.css" rel="stylesheet"/>
<script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
</head>
<body data-md-color-accent="light-blue" data-md-color-primary="black" data-md-color-scheme="default" dir="ltr">
<script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#evaluation">
          Skip to content
        </a>
</div>
<div data-md-component="announce">
</div>
<header class="md-header md-header--shadow" data-md-component="header">
<nav aria-label="Header" class="md-header__inner md-grid">
<a aria-label="Private Chatbot" class="md-header__button md-logo" data-md-component="logo" href="https://anote.ai" title="Private Chatbot">
<img alt="logo" src="../assets/logo.png"/>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            Private Chatbot
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              Evaluation
            
          </span>
</div>
</div>
</div>
<form class="md-header__option" data-md-component="palette">
<input aria-label="Switch to dark mode" class="md-option" data-md-color-accent="light-blue" data-md-color-media="" data-md-color-primary="black" data-md-color-scheme="default" id="__palette_1" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_2" hidden="" title="Switch to dark mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"></path></svg>
</label>
<input aria-label="Switch to light mode" class="md-option" data-md-color-accent="light-blue" data-md-color-media="" data-md-color-primary="black" data-md-color-scheme="slate" id="__palette_2" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_1" hidden="" title="Switch to light mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"></path></svg>
</label>
</form>
<label class="md-header__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="Search" required="" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg>
</label>
<nav aria-label="Search" class="md-search__options">
<button aria-label="Clear" class="md-search__icon md-icon" tabindex="-1" title="Clear" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"></path></svg>
</button>
</nav>
<div class="md-search__suggest" data-md-component="search-suggest"></div>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            Initializing search
          </div>
<ol class="md-search-result__list" role="presentation"></ol>
</div>
</div>
</div>
</div>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Navigation" class="md-nav md-nav--primary" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="Private Chatbot" class="md-nav__button md-logo" data-md-component="logo" href="https://anote.ai" title="Private Chatbot">
<img alt="logo" src="../assets/logo.png"/>
</a>
    Private Chatbot
  </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../privategpt/privategpt.html">
        Home
      </a>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_2" type="checkbox"/>
<label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          Anote SDK
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_2_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_2">
<span class="md-nav__icon md-icon"></span>
          Anote SDK
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_2_1" type="checkbox"/>
<label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
          Getting Started
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_2_1_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_2_1">
<span class="md-nav__icon md-icon"></span>
          Getting Started
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../api-overview/setup.html">
        Overview
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../privategpt/installation.html">
        Installation (Private)
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../api-overview/upload.html">
        Upload
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../api-overview/upload-private.html">
        Upload (Private)
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../api-overview/finetune.html">
        Fine Tune
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../api-overview/chat.html">
        Chat
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../api-overview/predict.html">
        Predict
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../api-overview/evaluate.html">
        Evaluate
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_2_2" type="checkbox"/>
<label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
          Capabilities
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_2_2_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_2_2">
<span class="md-nav__icon md-icon"></span>
          Capabilities
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../api-qa/example1.html">
        Q&amp;A from Edgar
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../api-qa/example2.html">
        Q&amp;A from Uploaded Files
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../api-qa/example3.html">
        Q&amp;A in Multiple Languages
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../api-qa/example4.html">
        Q&amp;A with websites
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../api-qa/example5.html">
        Q&amp;A with fine tuning
      </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_3" type="checkbox"/>
<label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          Anote 10-Ks
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_3_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_3">
<span class="md-nav__icon md-icon"></span>
          Anote 10-Ks
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../10-ks/10ksbackground.html">
        10-Ks Background
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../10-ks/10ksquestions.html">
        10-Ks Questions
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../10-ks/10ksdecomposition.html">
        10-Ks Decomposition
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../10-ks/10ksfeedback.html">
        10-Ks Feedback
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../10-ks/10kssolution.html">
        10-Ks Solution
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../10-ks/10ksimpact.html">
        10-Ks Impact
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_4" type="checkbox"/>
<label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          Anote Fine Tuning
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="true" aria-labelledby="__nav_4_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_4">
<span class="md-nav__icon md-icon"></span>
          Anote Fine Tuning
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="overview.html">
        Overview
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="example1.html">
        Baseline Models
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="example2.html">
        Supervised
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="example3.html">
        Supervised Methods
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="example4.html">
        Unsupervised
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="example5.html">
        RLHF
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="example6.html">
        Traditional RAG
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="example7.html">
        Enhanced RAG
      </a>
</li>
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
          Evaluation
          <span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="example8.html">
        Evaluation
      </a>
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#structured-vs-unstructured-evaluation">
    Structured vs. Unstructured Evaluation
  </a>
<nav aria-label="Structured vs. Unstructured Evaluation" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#structured-evaluation-answer-accuracy">
    Structured Evaluation Answer Accuracy
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#retrieval-accuracy">
    Retrieval Accuracy:
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#unstructured-evaluation-metrics">
    Unstructured Evaluation Metrics
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#identifying-anomalies-and-measuring-trustworthiness">
    Identifying Anomalies and Measuring Trustworthiness
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#aggregate-metrics">
    Aggregate Metrics
  </a>
<nav aria-label="Aggregate Metrics" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#financial-specific-metrics">
    Financial Specific Metrics
  </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#structured-vs-unstructured-evaluation">
    Structured vs. Unstructured Evaluation
  </a>
<nav aria-label="Structured vs. Unstructured Evaluation" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#structured-evaluation-answer-accuracy">
    Structured Evaluation Answer Accuracy
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#retrieval-accuracy">
    Retrieval Accuracy:
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#unstructured-evaluation-metrics">
    Unstructured Evaluation Metrics
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#identifying-anomalies-and-measuring-trustworthiness">
    Identifying Anomalies and Measuring Trustworthiness
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#aggregate-metrics">
    Aggregate Metrics
  </a>
<nav aria-label="Aggregate Metrics" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#financial-specific-metrics">
    Financial Specific Metrics
  </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<h1 id="evaluation">Evaluation</h1>
<p>Evaluation metrics are important to measure how each fine tuned model performs versus raw traditional models such as GPT4 or Llama3. We use evaluation metrics understand and quantify performance of the fine tuned LLMs, to ensure they perform accurately.</p>
<h2 id="structured-vs-unstructured-evaluation">Structured vs. Unstructured Evaluation</h2>
<p>When it comes to evaluating question and answering models on financial documents, the two main components are evaluating the model’s ability to retrieve context as well as its ability to answer questions based on the context. Ideally, we have access to ground truth context and answers developed by human financial analysts for <strong>structured evaluation</strong>.</p>
<table>
<thead>
<tr>
<th>Question</th>
<th>Human Answer</th>
<th>Human Chunk</th>
<th>Claude Model Answer</th>
<th>Claude Model Chunk</th>
<th>Llama Model Answer</th>
<th>Llama Model Chunk</th>
</tr>
</thead>
<tbody>
<tr>
<td>What is the total amount of the invoice?</td>
<td>$22,500.00</td>
<td>Total Amount $22,500.00</td>
<td>$22,500.00</td>
<td>total amount $22,500.00</td>
<td>$22,500.00</td>
<td>about Total Amount $22,500.00</td>
</tr>
<tr>
<td>What is the invoice number?</td>
<td>#0001</td>
<td>INVOICE # 0001</td>
<td>#0001</td>
<td>about INVOICE # 0001</td>
<td>#0001</td>
<td>INVOICE # 0001</td>
</tr>
<tr>
<td>What is a list of the items being purchased?</td>
<td>•Front End Engineering Service; •Back End Engineering Service; •Quality Assurance Manager</td>
<td>Front End Engineering Service $5000.00 Back End Engineering Service $7500.00 Quality Assurance Manager</td>
<td>•front end engineering service; •back end engineering service; •quality assurance manager</td>
<td>FRONT END ENGINEERING SERVICE $5000.00 BACK END ENGINEERING SERVICE $7500.00 QUALITY ASSURANCE MANAGER</td>
<td>•front end engineering service; •back end engineering service; •quality assurance manager</td>
<td>about Front End Engineering Service $5000.00 Back End Engineering Service $7500.00 Quality Assurance Manager</td>
</tr>
<tr>
<td>What is the name of the contact for question?</td>
<td>Bia Hermes</td>
<td>contact Bia Hermes</td>
<td>bia hermes</td>
<td>about contact Bia Hermes</td>
<td>Bia Hermes</td>
<td>contact bia hermes</td>
</tr>
<tr>
<td>What is the PO number?</td>
<td>#1000</td>
<td>P.O. # 1000</td>
<td>about #1000</td>
<td>P.O. # 1000</td>
<td>#1000</td>
<td>p.o. # 1000</td>
</tr>
<tr>
<td>When is payment due?</td>
<td>within 30 days of 01/01/2022</td>
<td>Payment is due within 30 days</td>
<td>WITHIN 30 DAYS OF 01/01/2022</td>
<td>PAYMENT IS DUE WITHIN 30 DAYS</td>
<td>WITHIN 30 DAYS OF 01/01/2022</td>
<td>Payment is due within 30 days</td>
</tr>
</tbody>
</table>
<p>Notice how there is human labeled answers and chunks above, so we can compare each model's answers and chunks from Claude and Llama3 to the human labeled answer (which we take as ground truth). However, in scenarios where we do not have access to this ground truth data, we need metrics that inform us about the quality of the chunk and answer from the model, hence <strong>unstructured evaluation</strong> .</p>
<table>
<thead>
<tr>
<th>Question</th>
<th>Claude Model Answer</th>
<th>Claude Model Chunk</th>
<th>Llama Model Answer</th>
<th>Llama Model Chunk</th>
</tr>
</thead>
<tbody>
<tr>
<td>What is the total amount of the invoice?</td>
<td>$22,500.00</td>
<td>total amount $22,500.00</td>
<td>$22,500.00</td>
<td>about Total Amount $22,500.00</td>
</tr>
<tr>
<td>What is the invoice number?</td>
<td>#0001</td>
<td>about INVOICE # 0001</td>
<td>#0001</td>
<td>INVOICE # 0001</td>
</tr>
<tr>
<td>What is a list of the items being purchased?</td>
<td>•front end engineering service; •back end engineering service; •quality assurance manager</td>
<td>FRONT END ENGINEERING SERVICE $5000.00 BACK END ENGINEERING SERVICE $7500.00 QUALITY ASSURANCE MANAGER</td>
<td>•front end engineering service; •back end engineering service; •quality assurance manager</td>
<td>about Front End Engineering Service $5000.00 Back End Engineering Service $7500.00 Quality Assurance Manager</td>
</tr>
<tr>
<td>What is the name of the contact for question?</td>
<td>bia hermes</td>
<td>about contact Bia Hermes</td>
<td>Bia Hermes</td>
<td>contact bia hermes</td>
</tr>
<tr>
<td>What is the PO number?</td>
<td>about #1000</td>
<td>P.O. # 1000</td>
<td>#1000</td>
<td>p.o. # 1000</td>
</tr>
<tr>
<td>When is payment due?</td>
<td>WITHIN 30 DAYS OF 01/01/2022</td>
<td>PAYMENT IS DUE WITHIN 30 DAYS</td>
<td>WITHIN 30 DAYS OF 01/01/2022</td>
<td>Payment is due within 30 days</td>
</tr>
</tbody>
</table>
<h3 id="structured-evaluation-answer-accuracy">Structured Evaluation Answer Accuracy</h3>
<p>Answer accuracy takes in the model answer and ground truth answer to evaluate whether or not the model is correct or not. For questions asking for a particular numerical metric, we can use a binary evaluation via regex expressions to say whether it is correct or not. However for open ended, explanation based questions in natural language, it can be more complicated, so we can utilize the following metrics for evaluating <strong>answer accuracy</strong>:</p>
<table>
<thead>
<tr>
<th align="left">Metrics</th>
<th align="left">Description</th>
<th align="left">Example of Calculation</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><code>LLM eval</code></td>
<td align="left">This metric serves as a substitute for human evaluation, where we can prompt a model like GPT-4 to see if two answers have the same semantic meaning, and prompt it to assign a specific score</td>
<td align="left">Use GPT-4 to evaluate the semantic similarity between "The sky is clear" and "It's a cloudless day" and assign a score.</td>
</tr>
<tr>
<td align="left"><code>Cosine Similarity</code></td>
<td align="left">This is a more automated way of comparing semantic meaning, however relies on both answers being extremely similar in order to have a high score</td>
<td align="left">Calculate the cosine similarity of the TF-IDF vectors for the sentences "I enjoy reading books" and "Reading books is enjoyable".</td>
</tr>
<tr>
<td align="left"><code>Rouge-L Score</code></td>
<td align="left">This metric is based on the longest common subsequence (LCS) between our model output and reference</td>
<td align="left">Calculate the Rouge-L score by finding the LCS of "The cat is sleeping on the mat" and "A cat sleeps on a mat".</td>
</tr>
<tr>
<td align="left"><code>Bleu Score</code></td>
<td align="left">This metric compares how similar two texts are as a number between 0 and 1. Generally a score of at least 0.6 means that two texts are similar enough to mean the same thing.</td>
<td align="left">Calculate the Bleu Score for machine translated text compared to a human reference translation to assess quality.</td>
</tr>
</tbody>
</table>
<p>However, for the use case of answering questions on many long financial documents, oftentimes the answers to questions can be wrong or incorrect. Sometimes it is due to more answers from the model, but more often than not it is due to the model thinking the answer is in the wrong part of the document. Because of this, retrieval systems such as RAG focus on identifying the right source / chunk of text to better inform the answer, that way the answer is coming from the right section in the document.</p>
<h3 id="retrieval-accuracy">Retrieval Accuracy:</h3>
<p>It is important to evaluate retrieval accuracy because if the chunk in the document that the model sources is incorrect, the answer to the question from the model is most likely going to be wrong.  Retrieval accuracy metric takes into account the context in addition to the answer as a way to evaluate how well the pipeline can retrieve the correct section. With access to the ground truth context, we can evaluate document level, page level and paragraph level, and multi-chunk level accuracies for retrieval.</p>
<table>
<thead>
<tr>
<th align="left">Metrics</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><code>document level</code></td>
<td align="left">This metric checks if retrieved chunk is on the same document in the document as the actual chunk</td>
</tr>
<tr>
<td align="left"><code>page level</code></td>
<td align="left">This metric checks if retrieved chunk is on the same page in the document as the actual chunk</td>
</tr>
<tr>
<td align="left"><code>paragraph level</code></td>
<td align="left">This metric checks if retrieved chunk is on the same paragraph in the document as the actual chunk</td>
</tr>
<tr>
<td align="left"><code>multi-chunk level</code></td>
<td align="left">This metric checks if multiple retrieved chunk are found in the same place in the document as the actual chunks</td>
</tr>
</tbody>
</table>
<p>Multi-chunk level accuracies is still a work in progress, and is very important for cases where answers to questions come from multiple chunks / pages / documents, where each chunk has contains an important part of information required for the answer. On the retrieval side, this is a limitation with RAG based systems that only find the top most similar chunk (where the similar chunk might not be the most relevant chunk).</p>
<h3 id="unstructured-evaluation-metrics">Unstructured Evaluation Metrics</h3>
<p>Without access to ground truth answers, most of the evaluation metrics will be based on whether or not the model answer is grounded in the context retrieved. These metrics are from the <a href="https://arxiv.org/pdf/2309.15217">RAGAS evaluation framework</a>.</p>
<table>
<thead>
<tr>
<th align="left">Metrics</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><code>Faithfulness</code></td>
<td align="left">This metrics evaluates whether the answer is supported by the given context, and penalizes the model if it hallucinated information not supported by the text.</td>
</tr>
<tr>
<td align="left"><code>Answer Relevance</code></td>
<td align="left">This metric evaluates whether or not the answer actually addresses the question. It does not account for accuracy, but penalizes for incomplete/redundant answers</td>
</tr>
</tbody>
</table>
<p>These are the metrics currently supported from within our public chatbot SDK, as when you use a chatbot such as Private Chatbot, Perplexity, GPT, Bard or Claude, there is no source of ground truth for model answers to be compared against.</p>
<h3 id="identifying-anomalies-and-measuring-trustworthiness">Identifying Anomalies and Measuring Trustworthiness</h3>
<p>Within the <a href="https://docs.anote.ai/structured/mislabelsprompting.html">Anote documentation</a>, we have resources in regards to identifying anomalies in answers to questions. We leverage cosine similarity to measure trustworthiness, to ensure the models output reliable results, and flag the rows where model answers appear to be incorrect.</p>
<h2 id="aggregate-metrics">Aggregate Metrics</h2>
<p>In addition to understanding how each specific row metric performed for each specific question, answer, chunk, it's important to have aggregate metrics across the entire testing dataset. Aggregate metrics are important for benchmarking different fine tuned models across an entire testing data corpus. This could look something like the following.</p>
<table>
<thead>
<tr>
<th align="left">Aggregate Evaluation Metrics</th>
<th align="left">Claude</th>
<th align="left">Open AI</th>
<th align="left">RLHF Model 10 Labels</th>
<th align="left">Unsupervised FT Model</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><code>Cosine Similarity Score</code></td>
<td align="left">0.778</td>
<td align="left">0.821</td>
<td align="left">0.621</td>
<td align="left">.580</td>
</tr>
<tr>
<td align="left"><code>Rouge-L Score</code></td>
<td align="left">0.824</td>
<td align="left">0.901</td>
<td align="left">0.780</td>
<td align="left">.618</td>
</tr>
<tr>
<td align="left"><code>LLM Evaluation Score</code></td>
<td align="left">0.802</td>
<td align="left">0.821</td>
<td align="left">0.650</td>
<td align="left">.838</td>
</tr>
</tbody>
</table>
<p>What is important to note is that model performance is determined by the evaluation metrics specified, so the user should be able to add metrics that they care about for their specific domain to measure model performance.</p>
<h3 id="financial-specific-metrics">Financial Specific Metrics</h3>
<p>For financial specific aggregate evaluation metrics, the end user might care about measuring Financial Domain Knowledge, Quantity Extraction and Program Synthesis. With Kensho Benchmarks, you can evaluate the performance of fine tuned models on the BizBench dataset with the following steps:</p>
<ol>
<li>Download CSVs of data from your Anote fine tuned models</li>
<li>Navigate to <a href="https://benchmarks.kensho.com/">Kensho Benchmarks</a> website</li>
<li>Sign into Kensho Benchmarks Portal</li>
<li>Upload submitted CSVs into Kensho Benchmarks</li>
<li>See model performance on the Kensho Benchmarks leaderboard.</li>
</ol>
</article>
</div>
<script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>
</div>
</main>
<footer class="md-footer">
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
      Material for MkDocs
    </a>
</div>
<div class="md-social">
<a class="md-social__link" href="https://www.linkedin.com/company/anote-ai/" rel="noopener" target="_blank" title="www.linkedin.com">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg>
</a>
<a class="md-social__link" href="https://anote-ai.medium.com/" rel="noopener" target="_blank" title="anote-ai.medium.com">
<svg viewbox="0 0 640 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M180.5 74.262C80.813 74.262 0 155.633 0 256s80.819 181.738 180.5 181.738S361 356.373 361 256 280.191 74.262 180.5 74.262Zm288.25 10.646c-49.845 0-90.245 76.619-90.245 171.095s40.406 171.1 90.251 171.1 90.251-76.619 90.251-171.1H559c0-94.503-40.4-171.095-90.248-171.095Zm139.506 17.821c-17.526 0-31.735 68.628-31.735 153.274s14.2 153.274 31.735 153.274S640 340.631 640 256c0-84.649-14.215-153.271-31.742-153.271Z"></path></svg>
</a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<script id="__config" type="application/json">{"base": "..", "features": ["search.suggest", "search.highlight", "content.tabs.link", "content.code.copy", "content.code.annotation"], "search": "../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
<script src="../assets/javascripts/bundle.b4d07000.min.js"></script>
<script>document$.subscribe(() => {
            window.update_swagger_ui_iframe_height = function (id) {
                var iFrameID = document.getElementById(id);
                if (iFrameID) {
                    full_height = (iFrameID.contentWindow.document.body.scrollHeight + 80) + "px";
                    iFrameID.height = full_height;
                    iFrameID.style.height = full_height;
                }
            }
        
            let iframe_id_list = []
            var iframes = document.getElementsByClassName("swagger-ui-iframe");
            for (var i = 0; i < iframes.length; i++) { 
                iframe_id_list.push(iframes[i].getAttribute("id"))
            }
        
            let ticking = true;
            
            document.addEventListener('scroll', function(e) {
                if (!ticking) {
                    window.requestAnimationFrame(()=> {
                        let half_vh = window.innerHeight/2;
                        for(var i = 0; i < iframe_id_list.length; i++) {
                            let element = document.getElementById(iframe_id_list[i])
                            if(element==null){
                                return
                            }
                            let diff = element.getBoundingClientRect().top
                            if(element.contentWindow.update_top_val){
                                element.contentWindow.update_top_val(half_vh - diff)
                            }
                        }
                        ticking = false;
                    });
                    ticking = true;
                }
            });
        
            const dark_scheme_name = "slate"
            
            window.scheme = document.body.getAttribute("data-md-color-scheme")
            const options = {
                attributeFilter: ['data-md-color-scheme'],
            };
            function color_scheme_callback(mutations) {
                for (let mutation of mutations) {
                    if (mutation.attributeName === "data-md-color-scheme") {
                        scheme = document.body.getAttribute("data-md-color-scheme")
                        var iframe_list = document.getElementsByClassName("swagger-ui-iframe")
                        for(var i = 0; i < iframe_list.length; i++) {
                            var ele = iframe_list.item(i);
                            if (ele) {
                                if (scheme === dark_scheme_name) {
                                    ele.contentWindow.enable_dark_mode();
                                } else {
                                    ele.contentWindow.disable_dark_mode();
                                }
                            }
                        }
                    }
                }
            }
            observer = new MutationObserver(color_scheme_callback);
            observer.observe(document.body, options);
            })</script></body>
</html>